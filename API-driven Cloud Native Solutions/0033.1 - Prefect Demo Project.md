### **ðŸ“Œ Prefect Demo Project: ETL Workflow**

This demo project will demonstrate how to use **Prefect** to create a simple ETL (Extract, Transform, Load) workflow that extracts data from a source, transforms it, and loads it into a destination.

---

### **Steps for the Demo Project**

#### **1. Install Prefect**

To get started with Prefect, first install it using **pip**:

```bash
pip install prefect
```

If you plan to use **Prefect Cloud**, you can install the Prefect Cloud package:

```bash
pip install prefect[cloud]
```

#### **2. Define Your Tasks and Flow**

We'll define three tasks in this example:

1. **Extract data**: Simulate extracting data from a source (e.g., database, file, or API).
2. **Transform data**: Modify or clean the extracted data.
3. **Load data**: Simulate loading the transformed data into a database.

```python
from prefect import task, Flow

# 1. Extract Task
@task
def extract_data():
    print("Extracting data...")
    data = "raw data"
    return data

# 2. Transform Task
@task
def transform_data(raw_data):
    print(f"Transforming data: {raw_data}")
    transformed_data = raw_data.upper()  # Example transformation (convert to uppercase)
    return transformed_data

# 3. Load Task
@task
def load_data(transformed_data):
    print(f"Loading data: {transformed_data}")
    # In a real-world scenario, here we'd save the data to a database or data warehouse.
    return True

# Define the ETL Flow
with Flow("ETL Workflow") as flow:
    raw_data = extract_data()
    transformed_data = transform_data(raw_data)
    load_data(transformed_data)
```

---

#### **3. Running the Flow Locally**

Now, we will run the flow locally with Prefectâ€™s **LocalExecutor**, which will execute tasks sequentially.

```python
# Running the flow locally
flow.run()
```

#### **4. Prefect Cloud Execution (Optional)**

If youâ€™re using **Prefect Cloud**, you can create an account, set up a project, and register the flow in the cloud. 

To register and run your flow in **Prefect Cloud**, follow these steps:

1. **Sign up** at [Prefect Cloud](https://www.prefect.io/cloud/).
2. **Create a project** in the Prefect Cloud dashboard.
3. **Authenticate** with your Prefect Cloud account in your local environment.

```bash
prefect auth login --key <YOUR_API_KEY>
```

4. **Register the flow** with Prefect Cloud:

```python
flow.register(project_name="ETL Project")  # Replace with your project name
```

5. Once registered, the flow will be available to run directly from **Prefect Cloud**.

---

#### **5. Task Dependencies**

Prefect automatically handles **task dependencies**. For example, `transform_data` will only execute after `extract_data` finishes, and `load_data` will execute after `transform_data`.

You can also specify dependencies explicitly if needed:

```python
transform_data.set_upstream(extract_data)
load_data.set_upstream(transform_data)
```

---

#### **6. Monitoring and Logging**

You can monitor the execution of your workflows in the Prefect UI:

- **Local Monitoring**: When running locally, Prefect will print logs directly to your terminal.
- **Prefect Cloud Monitoring**: In **Prefect Cloud**, you can view detailed logs, task statuses, and pipeline execution status via the dashboard.

---

### **ðŸ“Œ Full Prefect Demo Code Example**

```python
from prefect import task, Flow

# 1. Extract Task
@task
def extract_data():
    print("Extracting data...")
    data = "raw data"
    return data

# 2. Transform Task
@task
def transform_data(raw_data):
    print(f"Transforming data: {raw_data}")
    transformed_data = raw_data.upper()  # Example transformation (convert to uppercase)
    return transformed_data

# 3. Load Task
@task
def load_data(transformed_data):
    print(f"Loading data: {transformed_data}")
    # In a real-world scenario, here we'd save the data to a database or data warehouse.
    return True

# Define the ETL Flow
with Flow("ETL Workflow") as flow:
    raw_data = extract_data()
    transformed_data = transform_data(raw_data)
    load_data(transformed_data)

# Run the flow locally
if __name__ == "__main__":
    flow.run()  # This triggers the ETL pipeline locally

# For Prefect Cloud users:
# flow.register(project_name="ETL Project")  # Register with Prefect Cloud
# flow.run()
```

---

### **ðŸ“Œ Enhancements**

You can expand this simple ETL workflow by adding the following features:

- **Error Handling**: Use Prefectâ€™s built-in retry logic to handle failures.
- **Parameterization**: Pass parameters (like file names, table names, etc.) dynamically when running the flow.
- **Scheduling**: Schedule the ETL job to run at specific intervals using **Prefect Scheduler**.
- **Notifications**: Configure email or Slack notifications on task completion or failure.

---

### **ðŸ“Œ Conclusion**

**Prefect** makes it easy to build, schedule, and monitor **data workflows**. By breaking down the workflow into reusable tasks, it helps create maintainable, scalable data pipelines.
